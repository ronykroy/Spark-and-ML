{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python3-spark21", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1"
        }
    }, 
    "cells": [
        {
            "execution_count": 1, 
            "metadata": {}, 
            "source": "sc", 
            "outputs": [
                {
                    "execution_count": 1, 
                    "data": {
                        "text/plain": "<pyspark.context.SparkContext at 0x7f81f72455c0>"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "DSE has the spark context preconfigured....\nusually this is boiler plate activity .. on a bare metal installation of spark etc..", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 2, 
            "metadata": {}, 
            "source": "sc.version", 
            "outputs": [
                {
                    "execution_count": 2, 
                    "data": {
                        "text/plain": "'2.1.2'"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "    RDD: Reselient Distributed dataset is an immutable data structure.. i.e. you cant change them once set.. think of array@java..?\n    hence the multitude of examples that take one RDD and extract a subset of that RDD as a new RDD var and then work on it.. \n    retains the original RDD and the subset is smaller to work with ususally\n    In Apache Spark, all work is done by creating new RDDs, transforming existing RDDs, or using RDDs to compute results. When working with RDDs, the Spark driver application automatically distributes the work across the cluster", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 3, 
            "metadata": {}, 
            "source": "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 4, 
            "metadata": {}, 
            "source": "x_nbr_rdd = sc.parallelize(x) \n# parallelize data - > RDD\n# actions query RDD\n# transformation : manipulate data", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 5, 
            "metadata": {}, 
            "source": "x_nbr_rdd.first()", 
            "outputs": [
                {
                    "execution_count": 5, 
                    "data": {
                        "text/plain": "1"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 6, 
            "metadata": {}, 
            "source": "x_nbr_rdd.take(3)\n", 
            "outputs": [
                {
                    "execution_count": 6, 
                    "data": {
                        "text/plain": "[1, 2, 3]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 7, 
            "metadata": {}, 
            "source": "y = [\"Hello Earthlings\", \"My Name is Spock\"]", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 8, 
            "metadata": {}, 
            "source": "y_str_rdd = sc.parallelize(y)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 9, 
            "metadata": {}, 
            "source": "y_str_rdd.take(1)", 
            "outputs": [
                {
                    "execution_count": 9, 
                    "data": {
                        "text/plain": "['Hello Earthlings']"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 10, 
            "metadata": {}, 
            "source": "x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1) # cool use of map rather than for :D", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 11, 
            "metadata": {}, 
            "source": "# see all elements: rdd_name.collect()\nx_nbr_rdd_2.collect()", 
            "outputs": [
                {
                    "execution_count": 11, 
                    "data": {
                        "text/plain": "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "### collect throws up all the data.. even if there are a mn rows.. hence seldom seen in so examples", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 12, 
            "metadata": {}, 
            "source": "X = [\"1,2,3,4,5,6,7,8,9,10\"]\ny_rd = sc.parallelize(X)\n\nSum_rd = y_rd.map(lambda y: y.split(\",\")).map(lambda y: (int(y[2])+int(y[9])))", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 13, 
            "metadata": {}, 
            "source": "Sum_rd.first()", 
            "outputs": [
                {
                    "execution_count": 13, 
                    "data": {
                        "text/plain": "13"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 14, 
            "metadata": {}, 
            "source": "Words = [\"Hello Human. I'm Apache Spark and I love running analysis on data.\"]\nwords_rd = sc.parallelize(Words)\nwords_rd.first()", 
            "outputs": [
                {
                    "execution_count": 14, 
                    "data": {
                        "text/plain": "\"Hello Human. I'm Apache Spark and I love running analysis on data.\""
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 15, 
            "metadata": {}, 
            "source": "words_rd2 = words_rd.map(lambda line: line.split(\" \"))\nwords_rd2.first()", 
            "outputs": [
                {
                    "execution_count": 15, 
                    "data": {
                        "text/plain": "['Hello',\n 'Human.',\n \"I'm\",\n 'Apache',\n 'Spark',\n 'and',\n 'I',\n 'love',\n 'running',\n 'analysis',\n 'on',\n 'data.']"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 16, 
            "metadata": {}, 
            "source": "words_rd2.count()\n", 
            "outputs": [
                {
                    "execution_count": 16, 
                    "data": {
                        "text/plain": "1"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 17, 
            "metadata": {}, 
            "source": "words_rd.count()", 
            "outputs": [
                {
                    "execution_count": 17, 
                    "data": {
                        "text/plain": "1"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "    The RDD still has only one element..", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 18, 
            "metadata": {}, 
            "source": "words_rd3 = words_rd.flatMap(lambda line: line.split(\" \"))\nwords_rd3.take(6)", 
            "outputs": [
                {
                    "execution_count": 18, 
                    "data": {
                        "text/plain": "['Hello', 'Human.', \"I'm\", 'Apache', 'Spark', 'and']"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "     flatmap is used to give each element of the split output .. its own element in the RDD...", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 19, 
            "metadata": {}, 
            "source": "z = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]\nz_str_rdd = sc.parallelize(z)\nz_str_rdd.first()", 
            "outputs": [
                {
                    "execution_count": 19, 
                    "data": {
                        "text/plain": "'First,Line'"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 20, 
            "metadata": {}, 
            "source": "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))\nz_str_rdd_split_flatmap.collect()", 
            "outputs": [
                {
                    "execution_count": 20, 
                    "data": {
                        "text/plain": "['First', 'Line', 'Second', 'Line', 'and', 'Third', 'Line']"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 21, 
            "metadata": {}, 
            "source": "countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))\ncountWords.collect()\n# creating a tally of the words", 
            "outputs": [
                {
                    "execution_count": 21, 
                    "data": {
                        "text/plain": "[('First', 1),\n ('Line', 1),\n ('Second', 1),\n ('Line', 1),\n ('and', 1),\n ('Third', 1),\n ('Line', 1)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 22, 
            "metadata": {}, 
            "source": "#using the 1 marked against each owrd for aggregation\nfrom operator import add\ncountWords2 = countWords.reduceByKey(add)\ncountWords2.collect()", 
            "outputs": [
                {
                    "execution_count": 22, 
                    "data": {
                        "text/plain": "[('Line', 3), ('Second', 1), ('Third', 1), ('First', 1), ('and', 1)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 23, 
            "metadata": {}, 
            "source": "#filtering .. filters the said term\nwords_rd3 = z_str_rdd_split_flatmap.filter(lambda line: \"Second\" in line) # this rdd .. created above\n\nprint (\"The count of words \" + str(words_rd3.first())) # use brackets... pn py 3 print is a funciton and not a statement\nprint (\"Is: \" + str(words_rd3.count()))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The count of words Second\nIs: 1\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 24, 
            "metadata": {}, 
            "source": "!rm README.md* -f # ! for sys operations \n# remove thereadme fileif it already exists\n!wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/README.md", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "--2018-01-19 00:33:15--  https://raw.githubusercontent.com/carloapp2/SparkPOT/master/README.md\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3624 (3.5K) [text/plain]\nSaving to: \u2018README.md\u2019\n\n100%[======================================>] 3,624       --.-K/s   in 0s      \n\n2018-01-19 00:33:15 (26.7 MB/s) - \u2018README.md\u2019 saved [3624/3624]\n\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 25, 
            "metadata": {}, 
            "source": "textfile_rdd = sc.textFile(\"README.md\")\ntextfile_rdd.count()", 
            "outputs": [
                {
                    "execution_count": 25, 
                    "data": {
                        "text/plain": "98"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 26, 
            "metadata": {}, 
            "source": "Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\nSpark_lines.first() # FILTER FOR A LINE AND DISPLAY FIRST", 
            "outputs": [
                {
                    "execution_count": 26, 
                    "data": {
                        "text/plain": "'# Apache Spark'"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 27, 
            "metadata": {}, 
            "source": "print (\"The file README.md has \" + str(Spark_lines.count()) + \\\n\" of \" + str(textfile_rdd.count()) + \\\n\" Lines with word Spark in it.\")", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The file README.md has 19 of 98 Lines with word Spark in it.\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "    Run a flatMap transformation on the Spark_lines RDD and split on white spaces.\n    Create an RDD with key-value pairs where the first element of the tuple is the word and the second element is the number 1.\n    Run a reduceByKey method with the add function to count the number of instances of each word.\n    Filter the resulting RDD to keep only the elements that start with the word \"Spark\". In Python, the syntax to determine whether a string starts with a token is: string.startswith(\"token\")\n    Display the resulting list of elements that start with \"Spark\".", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 28, 
            "metadata": {}, 
            "source": "temp = Spark_lines.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1)).reduceByKey(add)\nfilter_fn = lambda x_y: x_y[0].startswith(\"Spark\")\n# SO : 27776026\n# Syntax issues transtitionaing from 2 -> 3\ntemp.filter(filter_fn).collect()", 
            "outputs": [
                {
                    "execution_count": 28, 
                    "data": {
                        "text/plain": "[('Spark](#building-spark).', 1),\n ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n ('Spark.', 1),\n ('Spark', 14),\n ('SparkPi', 2)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 29, 
            "metadata": {}, 
            "source": "filter_fn = lambda x_y: \"Spark\" in x_y[0] # general syntax for python 3.x a string contains a certain token\ntemp.filter(filter_fn).collect()", 
            "outputs": [
                {
                    "execution_count": 29, 
                    "data": {
                        "text/plain": "[('Spark](#building-spark).', 1),\n ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n ('Spark.', 1),\n ('Spark', 14),\n ('SparkPi', 2),\n ('tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).',\n  1)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "### now we try with some real data\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 30, 
            "metadata": {}, 
            "source": "!rm Scores.txt* -f\n!wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt\n \nRaw_Rdd = sc.textFile(\"Scores.txt\")\n\nSumScores = Raw_Rdd.map(lambda l: l.split(\",\")).\\\nmap(lambda v : (v[0], int(v[1])+int(v[2])+int(v[3])+int(v[4])))\n\nFinal = SumScores.map(lambda avg: (avg[0],avg[1],avg[1]/4.0))\n\nFinal.take(5)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "--2018-01-19 00:33:16--  https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75 [text/plain]\nSaving to: \u2018Scores.txt\u2019\n\n100%[======================================>] 75          --.-K/s   in 0s      \n\n2018-01-19 00:33:17 (9.13 MB/s) - \u2018Scores.txt\u2019 saved [75/75]\n\n"
                }, 
                {
                    "execution_count": 30, 
                    "data": {
                        "text/plain": "[('Carlo', 15, 3.75),\n ('Mokhtar', 15, 3.75),\n ('Jacques', 15, 3.75),\n ('Braden', 15, 3.75),\n ('Chris', 15, 3.75)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": [], 
            "cell_type": "code"
        }
    ], 
    "nbformat_minor": 1
}